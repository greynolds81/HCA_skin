{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import scanpy as sc\n",
    "import scrublet as scr\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from bbknn import bbknn\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from geosketch import gs\n",
    "from numpy import cov\n",
    "import scipy.cluster.hierarchy as spc\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn\n",
    "sc.settings.verbosity = 3  # verbosity: errors (0), warnings (1), info (2), hints (3)\n",
    "sc.settings.set_figure_params(dpi=80, color_map='viridis')\n",
    "sc.logging.print_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Introduce variables\n",
    "\n",
    "#name of first object (arbitrary)\n",
    "data1 = \"_Training\"\n",
    "Object1 = './overall_data/Healthy_all_data.h5ad'\n",
    "#provide categorical containing donor information\n",
    "donor_id = 'donor_id'\n",
    "#provide cateorical to join between datasets, this shoould be annotations for cells (obs col)\n",
    "cat1 = 'final'\n",
    "#provide an output path and a folder name to be created\n",
    "output = \"./logit_regression_out_v2/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start of processing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset 1\n",
    "adata_orig = sc.read(Object1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_orig.obs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get some basic processing done for the orig data\n",
    "sc.pp.neighbors(adata_orig,n_neighbors=15, n_pcs=50)\n",
    "sc.external.pp.bbknn(adata_orig, batch_key='donor_id', approx=True, metric='angular', copy=False, n_pcs=50, trim=None, n_trees=10, use_faiss=True, set_op_mix_ratio=1.0, local_connectivity=1)\n",
    "sc.tl.umap(adata_orig)\n",
    "sc.pl.umap(adata_orig,color = 'final')\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_compare(adata, train_x ,train_label,subset_predict, subset_train,penalty='l2',sparcity=0.2,col_name='predicted'):\n",
    "    #Define LR parameters\n",
    "    penalty='l2'\n",
    "    sparcity=0.2\n",
    "    lr = LogisticRegression(penalty=penalty, C=sparcity)\n",
    "\n",
    "    if train_x == 'X':\n",
    "        train_label = adata.obs[common_cat].values\n",
    "        train_label = train_label[subset_train]\n",
    "        train_x = adata.X\n",
    "        predict_x = train_x\n",
    "        train_x = train_x[subset_train, :]\n",
    "        predict_x = train_x\n",
    "        predict_x = predict_x[subset_predict]\n",
    "\n",
    "\n",
    "    elif train_x in adata.obsm.keys():\n",
    "        #Define training parameters\n",
    "        train_label = adata.obs[common_cat].values\n",
    "        train_label = train_label[subset_train]\n",
    "        train_x = adata.obsm[train_x]\n",
    "        predict_x = train_x\n",
    "        train_x = train_x[subset_train, :]\n",
    "\n",
    "        #Define Prediction parameters\n",
    "        predict_x = predict_x[subset_predict]\n",
    "        predict_x = pd.DataFrame(predict_x)\n",
    "        predict_x.index = adata.obs[subset_predict].index\n",
    "\n",
    "        \n",
    "    #Train predictive model\n",
    "    model = lr.fit(train_x, train_label)\n",
    "    lr.fit(train_x, train_label)\n",
    "    predict = lr.predict_proba(predict_x)\n",
    "\n",
    "    #Create prediction table and map to adata.obs\n",
    "    predict = lr.predict(predict_x)\n",
    "    predict = pd.DataFrame(predict)\n",
    "    predict.index = adata.obs[subset_predict].index\n",
    "    adata.obs[col_name] = adata.obs.index\n",
    "    adata.obs[col_name] = adata.obs[col_name].map(predict[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of predictive and output loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create output directory\n",
    "if os.path.exists(output) == True:\n",
    "    print(\"Path already exists!\")\n",
    "else:\n",
    "    os.mkdir(output)\n",
    "\n",
    "os.chdir('./'+output)\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "#Start of the subset and plotting loop\n",
    "for i in adata_orig.obs[donor_id].unique():\n",
    "#Create an on-memory copy of the original data for the loop\n",
    "    adata = adata_orig[:]\n",
    "    adata2 = adata[:]\n",
    "    adata2 = adata2[~adata2.obs[donor_id].isin([i])]\n",
    "#name of second object\n",
    "    data2 = \"_prediction\"\n",
    "#provide cateorical to join between datasets\n",
    "    cat2 = cat1\n",
    "    \n",
    "#create a common obs column in both datasets containing the data origin tag\n",
    "    common_cat = \"corr_concat\" \n",
    "    adata.obs[common_cat] = adata.obs[cat1].astype(str) + data1\n",
    "    adata2.obs[common_cat] = adata2.obs[cat2].astype(str) + data2\n",
    "    adata.obs = adata.obs.astype('category')\n",
    "    adata2.obs = adata2.obs.astype('category')\n",
    "    #adata.raw = adata\n",
    "    #adata2.raw = adata2\n",
    "\n",
    "#concat the data\n",
    "    concat = adata.concatenate(adata2, join='inner', index_unique='-', batch_key='Status')\n",
    "    adata = concat\n",
    "    print('data_concatenated!')\n",
    "#Get PCA by covarainces of variable genes\n",
    "    sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4)\n",
    "    sc.pp.log1p(adata)\n",
    "    #sc.pp.highly_variable_genes(adata, min_mean=0.1, max_mean=4)\n",
    "\n",
    "#%% PCA\n",
    "    sc.pp.pca(adata, n_comps=15, use_highly_variable=False, svd_solver='arpack')\n",
    "    \n",
    "#Define the seprator category in the column of interest, this works by partial matches and enables a-symmetric comparisons\n",
    "    Data1_group = data1\n",
    "    Data2_group = data2\n",
    "\n",
    "#Define the common .obs column between cancatinated data\n",
    "    common_cat = \"corr_concat\"\n",
    "\n",
    "#Define the resource to train and predict on, PCA or X or UMAP (#if you wish to use gene expression, train_x = 'X' or 'X_pca' , or 'X_umap'\n",
    "    #train_x = 'X'\n",
    "    train_x = 'X_pca'\n",
    "    ########################################################################################\n",
    "    group1 = (adata.obs[common_cat][adata.obs[common_cat].str.contains(Data1_group)]).unique()\n",
    "    group1 = list(group1)\n",
    "    group2 = (adata.obs[common_cat][adata.obs[common_cat].str.contains(Data2_group)]).unique()\n",
    "    group2 = list(group2)\n",
    "    subset_predict = np.array(adata.obs[common_cat].isin(group2))\n",
    "    subset_train = np.array(adata.obs[common_cat].isin(group1))\n",
    "    train_label = adata.obs[common_cat].values\n",
    "    #########################################################################################\n",
    "    \n",
    "#Run LR\n",
    "    LR_compare(adata, train_x,train_label,subset_predict, subset_train,penalty='l2',sparcity=0.2,col_name='predicted')\n",
    "    print('LR_predictions completed for loop ' + i + '!' )\n",
    "#Plot predictions in probability heatmap\n",
    "    x='predicted'\n",
    "    y = common_cat\n",
    "    y_attr = adata.obs[y]\n",
    "    x_attr = adata.obs[x]\n",
    "    crs_tbl = pd.crosstab(x_attr, y_attr)\n",
    "    #crs_tbl_test = crs_tbl\n",
    "    \n",
    "#Cross table removes all result which have 0 matches, add these back into table\n",
    "    vals = list(crs_tbl.index)\n",
    "    pred = pd.DataFrame(group1)\n",
    "    missing_vals = pred.iloc[:,0][~(pred.iloc[:,0].isin(vals))]\n",
    "    crs_tbl = crs_tbl.T\n",
    "    for missing in missing_vals:\n",
    "        crs_tbl[missing] = 0\n",
    "    crs_tbl = crs_tbl.reindex(sorted(crs_tbl.columns), axis=1)\n",
    "    crs_tbl = crs_tbl.T\n",
    "    crs_tbl = crs_tbl.reindex(sorted(crs_tbl.columns), axis=1)\n",
    "    for col in crs_tbl :\n",
    "        crs_tbl[col] = crs_tbl[col].div(crs_tbl[col].sum(axis=0)).multiply(100)\n",
    "\n",
    "#Optionally save this image!\n",
    "    #plot_df_heatmap(crs_tbl.T, cmap='coolwarm', rotation=90, figsize=figsize, vmin=20, vmax=70)\n",
    "    #pal = sns.diverging_palette(240, 10, n=10)\n",
    "    #g = sns.clustermap(crs_tbl.T, cmap=pal,vmin=20, vmax=70,linewidths=.5)\n",
    "    #g = sns.heatmap(crs_tbl, cmap=pal, vmin=0, vmax=100,linewidths=.5 ,center=50,square=True )\n",
    "    #plt.show();\n",
    "    \n",
    "#Save probability distribution\n",
    "    prob_out_name = \"./logist_prediction_prob_donor_minus_\" + i+ \".csv\"\n",
    "    crs_tbl.to_csv(prob_out_name)\n",
    "    print(\"LR probs saved for loop \" + i +\"!\")\n",
    "############################\n",
    "\n",
    "    #Crete a holder for current obj\n",
    "    adata3 = adata[:]\n",
    "    #load original preiction object\n",
    "    adata_predicted = adata2[:]\n",
    "    #Assign matches\n",
    "    adata3 = adata3[(adata3.obs[common_cat].isin(group2))]\n",
    "    adata3.obs[common_cat].unique()\n",
    "    adata_predicted.obs['predicted'] = adata_predicted.obs.index\n",
    "    adata3.obs.index = adata_predicted.obs.index\n",
    "    adata3.obs[common_cat].astype(str)\n",
    "    adata_predicted.obs['predicted'] = adata3.obs[\"predicted\"].astype(str)\n",
    "    \n",
    "#Frequency redistribution by additive assignment if reclustering is done\n",
    "    cluster_prediction = \"clus_prediction\"\n",
    "    clusters_reassign = \"leiden_res5\"\n",
    "    res= 5\n",
    "    lr_predicted_col = 'predicted'\n",
    "    \n",
    "##Plot Umap with Rand index and mutual info score\n",
    "    sc.pp.neighbors(adata_predicted,n_neighbors=15, n_pcs=50)\n",
    "    sc.pp.highly_variable_genes(adata_predicted, min_mean=0.1, max_mean=4)\n",
    "    sc.pp.pca(adata_predicted, n_comps=15, use_highly_variable=False, svd_solver='arpack')\n",
    "    sc.external.pp.bbknn(adata_predicted, batch_key='donor_id', approx=True, metric='angular', copy=False, n_pcs=15, trim=None, n_trees=10, use_faiss=True, set_op_mix_ratio=1.0, local_connectivity=1)\n",
    "    sc.tl.umap(adata_predicted)\n",
    "\n",
    "    sc.tl.leiden(adata_predicted, resolution= res, key_added= clusters_reassign, random_state=24, n_iterations=-1)\n",
    "    adata_predicted.obs[cluster_prediction] = adata_predicted.obs.index\n",
    "    for z in adata_predicted.obs[clusters_reassign].unique():\n",
    "        df = adata_predicted.obs\n",
    "        df = df[(df[clusters_reassign].isin([z]))]\n",
    "        df_count = pd.DataFrame(df[lr_predicted_col].value_counts())\n",
    "        freq_arranged = df_count.index\n",
    "        cat = freq_arranged[0]\n",
    "        df.loc[:,cluster_prediction] = cat\n",
    "        adata_predicted.obs.loc[adata_predicted.obs[clusters_reassign] == z, [cluster_prediction]] = cat\n",
    "    #adata_predicted.obs[cluster_prediction] = adata_predicted.obs['predicted']\n",
    "#caculate Rand and MI\n",
    "    rand=sklearn.metrics.adjusted_rand_score(list(adata_predicted.obs[cat1]), list(adata_predicted.obs[cluster_prediction]))\n",
    "    mi=sklearn.metrics.adjusted_mutual_info_score(list(adata_predicted.obs[cat1]), list(adata_predicted.obs[cluster_prediction]), average_method='arithmetic')\n",
    "#include info in plot title\n",
    "##Edit the predicted col and get all missing values\n",
    "    adata_predicted.obs[cluster_prediction] = adata_predicted.obs[cluster_prediction].str.replace('_Training', '', regex=True)\n",
    "    adata_predicted.obs[cluster_prediction]\n",
    "    fig_name = str(i) + \" Adj_Rnd= \" + str(rand) + \" Mut_info= \" + str(mi)\n",
    "    sc.pl.umap(adata_predicted,color=cluster_prediction,title = fig_name,save=(\"_\"+str(i)+\".png\"))\n",
    "    print(\"Umap, rand,Mi plotted for loop \" + i + \"!\")\n",
    "# prints the missing and additional elements in list2 into our dataframe\n",
    "    missing = set(list(adata_predicted.obs[cat1])).difference(list(adata_predicted.obs[cluster_prediction]))\n",
    "\n",
    "##Create data frame and populate with scoring metrics\n",
    "    temp_score = pd.DataFrame(columns=[i],dtype=object)\n",
    "    temp_score = pd.DataFrame(temp_score.T)\n",
    "    temp_score[\"adj_rand\"] = rand\n",
    "    temp_score[\"mutual_info\"] = mi\n",
    "    temp_score[\"missing_vals\"] = \"NAN\"\n",
    "    temp_score.at[i, \"missing_vals\"] = missing\n",
    "\n",
    "    if 'concat_score' in globals():\n",
    "        concat_score = pd.concat([concat_score, temp_score])\n",
    "    else:\n",
    "        concat_score = temp_score[:]\n",
    "    \n",
    "#Save file?\n",
    "    #save_file_predicted = \"./adata_predicted_donor_minus_\" + i\n",
    "    #adata.write(save_file_orig)\n",
    "    \n",
    "#Return resources back to system and restart the loop\n",
    "    #gc.collect()\n",
    "    del adata2\n",
    "    del adata3\n",
    "    print(\"end of loop!\")\n",
    "#Write out overall scored output\n",
    "concat_score.to_csv(\"./concatenated_scores_minus_donor.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workhorse_python3",
   "language": "python",
   "name": "workhorse_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
